{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Transformer Models\n",
    "\n",
    "- pipeline() function for tasks such as text generation and classification\n",
    "- Transformer architecture\n",
    "- encoder, decoder, and encoder-decoder architectures and use cases\n",
    "\n",
    "\n",
    "## Introduction \n",
    "\n",
    "\n",
    "NLP focuses on **understanding everything related to human language**. not only to understand **single words individually**, but to be able to understand the **context of those words**.\n",
    "\n",
    "**Use case examples:**\n",
    "\n",
    "- **Classifying whole sentences**:     \n",
    "    - sentiment analysis, spam detection, grammar correction    \n",
    "- **Classifying each word in a sentence**:     \n",
    "    - part-of-speech tagging or POS tagging: identifying the grammatical components of a sentence such as nouns, verbs, and adjectives and assigning the appropriate grammatical tag to each word).    \n",
    "    - entity recognition or NER: task of identifying and classifying named entities such as persons, locations, organizations, and other proper nouns in a text. \n",
    "\n",
    "- **Generating text content**:\n",
    "    This task is often used for text completion, sentence generation, or to assess a model's understanding and ability to generate coherent and contextually appropriate text. known as masked language modeling or cloze-style language modeling. In this task, a model is given a text with certain words or tokens masked or removed, and the model's objective is to predict or generate the missing words or tokens.\n",
    "\n",
    "- **Extracting an answer from a text**:   \n",
    "    Question answering which can be extractive and abstractive:\n",
    "    - In extractive question answering, the model identifies and selects a span of text from the context that directly answers the question. The selected span is typically a contiguous sequence of words or tokens from the context.\n",
    "\n",
    "    - In abstractive question answering, the model generates a concise and coherent answer to the question based on the information in the context. The generated answer may not be an exact span of text from the context but rather a paraphrased or synthesized response.\n",
    "\n",
    "- **Generating a new sentence from an input text**: machine translation (MT) and text summarization\n",
    "\n",
    "\n",
    "NLP doesn't only deal with written text. It also works on understanding and solving difficult problems related to speech recognition and computer vision. For example, it can generate a written version of an audio recording or describe what's happening in an image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with pipelines\n",
    "\n",
    "Transformer models are used to solve all kinds of NLP tasks. The HF Transformers library provides the functionality to **create** and **use the models** that have been shared by researchers. \n",
    "\n",
    "**pipeline() function** is the most basic object in the libaray, it connects a model with its necessary preprocessing and postprocessing steps."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the pipeline directly to input any text and get an output. The following code shows this with an example sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code **creates a pipeline** for **sentiment analysis** using the **pipeline function** from the **transformers module**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same with multiple sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9996962547302246},\n",
       " {'label': 'NEGATIVE', 'score': 0.9997852444648743}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\n",
    "    [\"I've been feeling sick latley.\", \"This NLP project is sick!\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the model didnt pick up the meaning of word sick, which is a slang, in the second sentence. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happens when we pass some text to the pipline?**\n",
    "\n",
    "- The input data is being preprocessed for the model. that is the text is preprocessed into a format the model can understand. \n",
    "- The preprocessed inputs are passed to the model.\n",
    "- The predictions of the model are post-processed, so we can understand them.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Pipeline\n",
    "\n",
    "https://huggingface.co/docs/transformers/pipeline_tutorial \n",
    "\n",
    "\n",
    "There are two categories of pipeline abstractions to be aware about:\n",
    "\n",
    "- 1. **The pipeline()** which is the most powerful object encapsulating all other pipelines.\n",
    "- 2. **Task-specific pipelines** are available for audio, computer vision, natural language processing, and multimodal tasks.\n",
    "\n",
    "For NLP tasks there are different available pipelines, we will look at some of them:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot classification \n",
    "\n",
    "**Objective:** classify texts that haven’t been labelled.\n",
    "- The zero-shot-classification pipeline allows you to **classify text into multiple candidate labels even if those labels are not present in the training data**.\n",
    "- This pipeline is called zero-shot because you don’t need to fine-tune the model on your data to use it. It can directly return probability scores for any list of labels you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.15k/1.15k [00:00<00:00, 2.91MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 1.63G/1.63G [00:55<00:00, 29.5MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 71.2kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 10.1MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 8.33MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 12.7MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.8445996642112732, 0.11197379231452942, 0.04342653229832649]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: as you can see when no model is specified, the pipline uses the default model which in this case is facebook/bart-large-mnli.          \n",
    "We can use the following models:\n",
    "\n",
    "1. **facebook/bart-large-mnli**: This model is a **BART model fine-tuned on the MNLI** (Multi-Genre Natural Language Inference) dataset. It is capable of zero-shot classification tasks.\n",
    "\n",
    "2. **roberta-large-mnli**: This model is a **RoBERTa model fine-tuned on the MNLI dataset**. It is also suitable for zero-shot classification.\n",
    "\n",
    "3. **distilbert-base-uncased**: This model is a **DistilBERT model** trained on the uncased version of the English text. It is a **smaller and faster variant of BERT** and can be used for zero-shot classification.\n",
    "\n",
    "Now lets do the same but this time with a specified model, roberta-large-mnli:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 688/688 [00:00<00:00, 1.06MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 1.43G/1.43G [00:29<00:00, 48.2MB/s]\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 15.2MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 16.1MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 47.8MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.9562345743179321, 0.026972180232405663, 0.016793299466371536]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"roberta-large-mnli\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does the scores output mean here?**      \n",
    "The scores represent the **confidence scores** assigned to each of the candidate labels provided in the classifier call. These scores indicate the model's confidence in the predicted probability of the input text belonging to each of the candidate labels. The scores are typically normalized probabilities, meaning they add up to 1 across all labels.\n",
    "In this example, the highest score of 0.9562345743179321 is assigned to the label 'education', indicating that the model believes the input text is most likely related to education compared to the other candidate labels. The lower scores for the remaining labels suggest lower confidence in those classifications.\n",
    "\n",
    "The scores can be useful for understanding the model's level of certainty in its predictions and can be utilized to make decisions based on the classification confidence thresholds that best suit any specific application."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation\n",
    "\n",
    "**Objective:** inputing a prompt, the model will auto-complete it by generating the remaining text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this news article, \\xa0Jai and other contributors discuss at length the reasons for the increased demand for medical facilities in Indonesia. The majority blame the price of the medications used to treat the disease for the increased demand and increase in the number of'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"In this news article, \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The pipiline defaulted to gpt2 when not given any model. Text-generation pipeline can use various pre-trained models depending on the version of the transformers library. A few examples of models we can use with the text-generation pipeline are gpt2 (Generative Pretrained Transformer 2) model, gpt2-medium, gpt2-large, and distilgpt2 (a smaller and faster variant of the GPT-2 model called DistilGPT-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 762/762 [00:00<00:00, 1.85MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 353M/353M [00:07<00:00, 49.2MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 780kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 26.6MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 28.0MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 21.0MB/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this news article, izar is a freelance reporter. He writes about global politics in the Washington Post for the Center for American Progress and has'},\n",
       " {'generated_text': 'In this news article, \\xa0 is only one of his recent claims that it is true that the U.S. Constitution actually prohibits the practice of'},\n",
       " {'generated_text': 'In this news article, ______________________________________________\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'},\n",
       " {'generated_text': 'In this news article, 今漀便楮人, which came out after the World War I ended, would'},\n",
       " {'generated_text': 'In this news article, \\ue601 \\ue60a \\ue60a \\ue60a \\ue60a \\ue60a '}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\", num_return_sequences= 5 , max_length=30)\n",
    "generator(\"In this news article, \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahnaz/vscodeProjects/HF-NLP-Notes/venv_HF-NLP-Notes/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to install and modify the Arduino IDE using Arduino on an Arduino computer.\\n\\n\\n\\nIn any case'},\n",
       " {'generated_text': 'In this course, we will teach you how to start your own family life and the future with a personal story.'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using any model from the Hub in a pipeline\n",
    "\n",
    "To find other models to use with the pipeline: go to the HuggingFace **Model Hub** and click on the corresponding tag (e.g here text generation) on the left to display only the supported models for that task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask filling\n",
    "\n",
    "**Objective:** filling in the blanks in a given text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.020203422755002975,\n",
       "  'token': 7967,\n",
       "  'token_str': ' survival',\n",
       "  'sequence': 'life is all about survival.'},\n",
       " {'score': 0.01898636482656002,\n",
       "  'token': 657,\n",
       "  'token_str': ' love',\n",
       "  'sequence': 'life is all about love.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"life is all about <mask> .\", top_k=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **top_k** argument controls how many possibilities you want to be displayed. The special <mask> word is often referred to as a mask token."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition\n",
    "\n",
    "**Objective:** finding which parts of the input text correspond to entities such as persons, locations, or organizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 998/998 [00:00<00:00, 2.07MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.33G/1.33G [00:30<00:00, 43.8MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 60.0/60.0 [00:00<00:00, 101kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 17.5MB/s]\n",
      "/Users/mahnaz/vscodeProjects/HF-NLP-Notes/venv_HF-NLP-Notes/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9981694,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9796019,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9932106,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**grouped_entities=True**: to tell the pipeline to regroup together the parts of the sentence that correspond to the same entity. e.g here the model correctly grouped “Hugging” and “Face” as a single organization, even though the name consists of multiple words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question answering\n",
    "\n",
    "**Objective:** answering questions using information from a given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.6483435034751892, 'start': 34, 'end': 45, 'answer': 'ML engineer'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Where do I do?\",\n",
    "    context=\"My name is Mahnaz and I work as a ML engineer.\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this pipeline **does not generate the answer**, it only works by **extracting information** from the **provided context**;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "\n",
    "**Objective:** reducing a text into a shorter text while keeping all (or most) of the important aspects referenced in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.80k/1.80k [00:00<00:00, 2.87MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.22G/1.22G [00:48<00:00, 25.2MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 42.3kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 24.3MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 14.2MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' A key feature of Transformer models is that they are built with special layers called attention layers . This layer tells the model to pay specific attention to certain words in the sentence you passed it (and more or less ignore the others) when dealing with the representation of each word . To put this into context, consider the task of translating text from English to French .'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\n",
    "    \"\"\"A key feature of Transformer models is that they are built with special layers called attention layers. In fact, the title \n",
    "    of the paper introducing the Transformer architecture was “Attention Is All You Need”! We will explore the details of attention \n",
    "    layers later in the course; for now, all you need to know is that this layer will tell the model to pay specific attention to\n",
    "      certain words in the sentence you passed it (and more or less ignore the others) when dealing with the representation of each word.\n",
    "      To put this into context, consider the task of translating text from English to French. Given the input “You like this course”, a \n",
    "      translation model will need to also attend to the adjacent word “You” to get the proper translation for the word “like”, because \n",
    "      in French the verb “like” is conjugated differently depending on the subject. The rest of the sentence, however, is not useful for\n",
    "        the translation of that word. In the same vein, when translating “this” the model will also need to pay attention to the word “course”,\n",
    "          because “this” translates differently depending on whether the associated noun is masculine or feminine. Again, the other words in \n",
    "          the sentence will not matter for the translation of “this”. With more complex sentences (and more complex grammar rules), the model\n",
    "            would need to pay special attention to words that might appear farther away in the sentence to properly translate each word.\n",
    "The same concept applies to any task associated with natural language: a word by itself has a meaning, but that meaning is deeply affected\n",
    " by the context, which can be any other word (or words) before or after the word being studied.\"\n",
    "\"\"\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify a max_length or a min_length for the result."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.42k/1.42k [00:00<00:00, 3.50MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 301M/301M [00:07<00:00, 41.1MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 293/293 [00:00<00:00, 1.17MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 62.2kB/s]\n",
      "Downloading (…)olve/main/source.spm: 100%|██████████| 802k/802k [00:00<00:00, 22.5MB/s]\n",
      "Downloading (…)olve/main/target.spm: 100%|██████████| 778k/778k [00:00<00:00, 59.4MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.34M/1.34M [00:00<00:00, 45.5MB/s]\n",
      "/Users/mahnaz/vscodeProjects/HF-NLP-Notes/venv_HF-NLP-Notes/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'This course is produced by Hugging Face.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "translator(\"Ce cours est produit par Hugging Face.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason these models dont work very well is that they were programmed for specific tasks and cannot perform variations of them. In order to get a better result we should customize the behaviour of pipeline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do Transformers work?\n",
    "\n",
    "Transformers go back to 2017 when the papare **Attention Is All You Need**, focused on translation tasks, was published. \n",
    "This was followed by the introduction of several influential models:\n",
    "\n",
    "- **GPT-like** (aka auto-regressive Transformer models)\n",
    "- **BERT-like** (aka called auto-encoding Transformer models)\n",
    "- **BART/T5-like** (aka sequence-to-sequence Transformer models)\n",
    "\n",
    "Some of the highlights:\n",
    "\n",
    "- June 2018: **GPT**, the **first pretrained Transformer model**, used for fine-tuning on **various NLP tasks** and obtained state-of-the-art results\n",
    "\n",
    "- October 2018: **BERT**, another large pretrained model, this one designed to produce better **summaries of sentences**(more on this in the next chapter!)\n",
    "\n",
    "- February 2019: **GPT-2**, an improved (and bigger) version of GPT that was not immediately publicly released due to ethical concerns\n",
    "\n",
    "- October 2019: **DistilBERT**, a distilled version of BERT that is 60% faster, 40% lighter in memory, and still retains 97% of BERT’s performance\n",
    "\n",
    "- October 2019: **BART** and **T5**, two large pretrained models using the same architecture as the original Transformer model (the first to do so)\n",
    "\n",
    "- May 2020, **GPT-3**, an even **bigger version of GPT-2** that is able to perform well on a variety of tasks **without the need for fine-tuning** (called zero-shot learning)\n",
    "\n",
    "\n",
    "Transformer models are language models, meaning they have been **trained on large amounts of raw text** in a **self-supervised** fashion. in self-supervised learning the **objective is automatically computed** **from the inputs** of the model. That means that humans are not needed to label the data!\n",
    "\n",
    "- These models develops a **statistical understanding of the language** it's trained on but lacks usefulness for specific practical tasks.\n",
    "- **Transfer learning** addresses this issue by **fine-tuning a pretrained model on a specific task**.\n",
    "- Fine-tuning involves supervised learning using human-annotated labels.\n",
    "- The process helps the model become more practical and task-oriented.\n",
    "\n",
    "Two examples of LLMs:    \n",
    "\n",
    "1. **Causal language modeling** is an example of a task where the goal is to **predict the next word** in a sentence based on the preceding n words. The output of this task depends on the past and present inputs, but not on future inputs.\n",
    "\n",
    "2. **Masked language modeling** in which the model predicts a masked word in the sentence.\n",
    "\n",
    "Ways to achieve better performance in transformers (except a few outliers like DistilBERT):  \n",
    "\n",
    "1. increasing the models’ sizes, \n",
    "2. as well as increasing the amount of data they are pretrained on.\n",
    "\n",
    "This becomes very costly in terms of time, compute resources, and environmental impacts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "Some definitions:\n",
    "\n",
    "- **Pretraining**:    \n",
    "    - training a model from scratch.\n",
    "    - training starts without any prior knowledge, weights are randomly initialized.\n",
    "    - usually done on very large amounts of data,\n",
    "    - requires a very large corpus of data, \n",
    "    - training can take up to several weeks.\n",
    "\n",
    "- **Fine-tuning**:\n",
    "    - training done after a model has been pretrained. \n",
    "    - get the pretrained model, then do additional training with a dataset specific to your task.\n",
    "\n",
    "Why not simply train directly for the final task, instead og fine-tuning?     \n",
    "- Since the pretrained model was trained on a dataset that shares similarities with the fine-tuning dataset, the fine-tuning process can leverage the knowledge gained by the initial model during pretraining.\n",
    "- fine-tuning requires way less data to get decent results.\n",
    "- fine-tuning requires way less  time and resources.\n",
    "\n",
    "Fine-tuning a model therefore has **lower time, data, financial, and environmental costs**. It's also easier to iterate over different fine-tuning schemes, also achieve better results than training from scratch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General architecture\n",
    "\n",
    "The model is primarily composed of two blocks:\n",
    "\n",
    "- **Encoder**: The encoder **receives an input** and **builds a representation of it** (its features). This means that the model is **optimized to acquire understanding from the input.**       \n",
    "\n",
    "- **Decoder**: The decoder **uses the encoder’s representation** (features) along with other inputs to **generate a target sequence.** This means that the model is **optimized for generating outputs**.\n",
    "\n",
    "Each of these parts can be used independently, depending on the task:\n",
    "\n",
    "- Encoder-only models: Good for tasks that require understanding of the input, such as sentence classification and named entity recognition.       \n",
    "- Decoder-only models: Good for generative tasks such as text generation.     \n",
    "- Encoder-decoder models or sequence-to-sequence models: Good for generative tasks that require an input, such as translation or summarization.        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention layers\n",
    "\n",
    "This layer will tell the model to pay specific attention to certain words in the sentence you passed it (and more or less ignore the others) when dealing with the representation of each word.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The original architecture\n",
    "\n",
    "The Transformer architecture was **originally designed for translation** and it works as follow:\n",
    "\n",
    "During **training**: \n",
    "\n",
    " - **encoder**: receives inputs (sentences) in a **certain language,**\n",
    "    - the attention layers can use all the words in a sentence (since the translation of a given word can be dependent on what is after as well as before it in the sentence).\n",
    " - **decoder**: receives the **same sentences** in the desired **target language.** \n",
    "    - works sequentially, the attention layers can only use the words in the sentence that it has already translated (so, only the words before the word currently being generated).\n",
    "\n",
    "To speed things up another layer of attention is added to decoder:   \n",
    "\n",
    "during training (when the model has access to target sentences), the decoder is fed the whole target, but it is not allowed to use future words (if it had access to the word at position 2 when trying to predict the word at position 2, the problem would not be very hard!).\n",
    "\n",
    "**attention layers in decoder**\n",
    "- First attention layer in decoder: Considers all past inputs\n",
    "- Second attention layer in decoder: Utilizes encoder's output\n",
    "- Second attention layer accesses whole input sentence for accurate predictions\n",
    "- Helpful for languages with different word orders or when context is important\n",
    "\n",
    "**attention mask**:\n",
    "can  be used in the encoder/decoder to **prevent the model from paying attention to some special words**.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectures vs. checkpoints\n",
    "\n",
    "Some termonology in Transformer models:\n",
    "\n",
    "- **Architecture**: skeleton of the model — the definition of each **layer** and each **operation** that happens within the model.\n",
    "- **Checkpoints**: These are the **weights** that will be loaded in a given architecture.\n",
    "- **Model**: This is an **umbrella term** that isn’t as precise as “architecture” or “checkpoint”: it can mean both.     \n",
    "For example, BERT is an architecture while bert-base-cased, a set of weights trained by the Google team for the first release of BERT, is a checkpoint. However, one can say “the BERT model” and “the bert-base-cased model.”\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_HF-NLP-Notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e271ed26a3bb9b9590c7e20b926699e380aa6724b9334935ca4b4823a968782"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
