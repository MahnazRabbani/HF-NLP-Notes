{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "Gaol of HUggingFace transformers libaray:    \n",
    "\n",
    "Its goal is to provide a **single API** through which any **Transformer model can be loaded, trained, and saved**.       \n",
    "The libraryâ€™s main features:\n",
    "\n",
    "- Downloading, loading, and using a state-of-the-art NLP model for inference is made easy with just **two lines of code**.\n",
    "- The models are built using PyTorch nn.Module or TensorFlow tf.keras.Model classes, making them flexible and compatible with their respective ML frameworks.\n",
    "- The library emphasizes simplicity by **minimizing abstractions**.\n",
    "- The **\"All in one file\" concept** is central, where a model's forward pass is defined in a single file, ensuring code readability and modifiability.\n",
    "\n",
    "\n",
    "The ðŸ¤— Transformers has a very unique feature: unlike traditional ML libraries where models are constructed using shared modules across different files, in ðŸ¤— Transformers, **each model has its own distinct layers**.       \n",
    "\n",
    "This approach offers several advantages:\n",
    " \n",
    "- Firstly, it makes the models more **accessible** and **comprehensible**, as the code for each model is self-contained and easier to understand. \n",
    "- Secondly, it provides the **flexibility to experiment** and **modify specific models without impacting others**. You can make changes to one model's layers or configurations without affecting the functionality or performance of other models.\n",
    "\n",
    "By adopting this approach, ðŸ¤— Transformers enhances the **modularity**, ease of **experimentation**, and overall **usability** of the library."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behind the pipeline\n",
    "\n",
    "Pipeline groups together three steps:\n",
    "\n",
    "1. preprocessing, done with tokenizer,\n",
    "2. passing the inputs through the model,\n",
    "3. postprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing with a tokenizer\n",
    "\n",
    "Modles canâ€™t process raw text directly, so we use a tokenizer to convert the text inputs into numbers that the model can make sense of. Tokenizer do the following:\n",
    "\n",
    "- **Splitting** the input into **words, subwords, or symbols** (like punctuation) that are **called tokens**\n",
    "- **Mapping** each token to an **integer**\n",
    "- Adding **additional inputs** that may be useful to the model     \n",
    "\n",
    "Important note: all this **preprocessing** must be done in **exactly the same way as when the model was pretrained**.\n",
    "To do so we use **AutoTokenizer** class and its **from_pretrained()** method to download that information. Using the **checkpoint name** of our model, it will automatically fetch the data associated with the modelâ€™s tokenizer and cache it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahnaz/vscodeProjects/HF-NLP-Notes/venv_HF-NLP-Notes/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# the default checkpoint of the sentiment-analysis pipeline is distilbert-base-uncased-finetuned-sst-2-english\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the tokenizer, we can directly pass our sentences to it and **weâ€™ll get back a dictionary** thatâ€™s ready to feed to our model! The only thing left to do is to convert the list of input IDs to tensors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformer** models **only accept tensors as input**. To **specify the type of tensors** we want to get back (PyTorch, TensorFlow, or plain NumPy), we use the return_tensors argument.\n",
    "\n",
    "We can pass one **sentence or a list of sentences,** as well as specifying the type of tensors we want to get back (if no type is passed, you will get a list of lists as a result).    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "2\n",
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(type(inputs))\n",
    "print(len(inputs))  \n",
    "print(inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of tokenizer is a **dictionary** containing **two keys**:    \n",
    "1. **input_ids**: contains two rows of integers (one for each sentence) that are the unique identifiers of the tokens in each sentence. \n",
    "2. **attention_mask**: we'll see what they are later."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going through the model\n",
    "\n",
    "We download our pretrained model using **AutoModel class** which also has a from_pretrained() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This architecture **contains only the base Transformer module**: given some inputs, it **outputs what weâ€™ll call hidden states**, also known as **features**. For each model input, weâ€™ll retrieve a **high-dimensional vector** representing the **contextual understanding** of that input by the Transformer model.\n",
    "\n",
    "- The hidden states, although valuable on their own, typically serve as inputs to another component of the model called the head. \n",
    "- Various tasks are accomplished using a **shared architecture**, but **each task had its own unique associated head**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A high-dimensional vector?\n",
    "\n",
    "**Vector output** by the Transformer module is usually large. It generally has three dimensions:   \n",
    "\n",
    "- **Batch size**: The **number of sequences processed** at a time (2 in our example).\n",
    "- **Sequence length**: The **length of the numerical representation** of the sequence (16 in our example).\n",
    "- **Hidden size**: The **vector dimension** of each model input.\n",
    "\n",
    "It is said to be **â€œhigh dimensionalâ€ because of the last value**. \n",
    "The hidden size can be very large (768 is common for smaller models, and in larger models this can reach 3072 or more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outputs of ðŸ¤— Transformers models behave like namedtuples or **dictionaries**. You can access the elements **by attributes** (like we did) or **by key** (outputs[\"last_hidden_state\"]), or even **by index** if you know exactly where the thing you are looking for is (outputs[0])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['last_hidden_state'])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model heads: Making sense out of numbers\n",
    "\n",
    "Model heads take high-dimensional hidden states as input and project them onto a different dimension.\n",
    "Model heads typically consist of one or a few linear layers.      \n",
    "\n",
    "The model is as follwoing:    \n",
    "\n",
    "Model input --> Embeddings --> Layers --> Hidden states --> Head --> Model output\n",
    "\n",
    "- Full model includes:  Embeddings --> Layers --> Hidden states --> Head       \n",
    "- Transformer network: Embeddings --> Layers       \n",
    "\n",
    "- The **output of the Transformer** model is **directly sent to the model head** for processing.\n",
    "- The **embeddings** layer converts **input IDs into token** vectors and the subsequent layers use attention mechanisms to manipulate the vectors and generate the final sentence representation.\n",
    "\n",
    "- There are many **different architectures** available in ðŸ¤— Transformers, with **each one designed around tackling a specific task.** Here is a non-exhaustive list:\n",
    "\n",
    "- *Model (retrieve the hidden states)     \n",
    "- *ForCausalLM      \n",
    "- *ForMaskedLM     \n",
    "- *ForMultipleChoice     \n",
    "- *ForQuestionAnswering       \n",
    "- *ForSequenceClassification      \n",
    "- *ForTokenClassification       \n",
    "- and others\n",
    "\n",
    "For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we wonâ€™t actually use the AutoModel class, but AutoModelForSequenceClassification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model head takes as input the high-dimensional vectors we saw before, and outputs vectors containing two values (one per label):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have just two sentences and two labels, the result we get from our model is of shape 2 x 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing the output\n",
    "\n",
    "Values we get as output from our model donâ€™t necessarily make sense by themselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9743, -1.6959],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model predicted [-1.5607, 1.6123] for the first sentence and [ 4.1692, -3.3464] for the second one. Those are not probabilities but **logits**, the **raw, unnormalized scores outputted by the last layer of the model**. To be **converted to probabilities,** they need to go through a **SoftMax layer** (all ðŸ¤— Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.7516e-01, 2.4838e-02],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the model predicted [0.0402, 0.9598] for the first sentence and [0.9995, 0.0005] for the second one. These are recognizable probability scores.\n",
    "\n",
    "To get the labels corresponding to each position, we can inspect the id2label attribute of the model config (more on this in the next section):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can conclude that the model predicted the following:      \n",
    "\n",
    "First sentence: NEGATIVE: 0.0402, POSITIVE: 0.9598         \n",
    "Second sentence: NEGATIVE: 0.9995, POSITIVE: 0.0005        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The AutoModel class and all of its relatives are actually simple **wrappers** over the wide variety of models available in the library.\n",
    "- It can automatically guess the appropriate model architecture for your checkpoint, and then instantiates a model with this architecture.\n",
    "\n",
    "However, if you know the type of model you want to use, you can use the class that defines its architecture directly. Letâ€™s take a look at how this works with a BERT model. The first thing weâ€™ll need to do to initialize a BERT model is load a configuration object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahnaz/vscodeProjects/HF-NLP-Notes/venv_HF-NLP-Notes/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = BertModel(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration contains many attributes that are used to build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hidden_size** attribute defines the **size of the hidden_states vector**, \n",
    "**num_hidden_layers** defines the **number of layers** the Transformer model has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "config = BertConfig()\n",
    "model = BertModel(config)\n",
    "\n",
    "# Model is randomly initialized!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be used in this state, but it will output gibberish; it needs to be trained first. But we reuse models that have already been trained.      \n",
    "\n",
    "Loading a Transformer model that is already trained is simple â€” we can do this using the **from_pretrained()** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436M/436M [00:08<00:00, 53.8MB/s] \n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is now initialized with all the weights of the checkpoint. It can be used directly for **inference on the tasks it was trained on**, and it can also be **fine-tuned on a new task.** By training with pretrained weights rather than from scratch, we can quickly achieve good results.   \n",
    "\n",
    "The identifier used to load the model can be the identifier of any model on the Model Hub, as long as it is compatible with the BERT architecture."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"directory_on_my_computer\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This **saves** **two files** to your disk:     \n",
    "\n",
    "ls directory_on_my_computer        \n",
    "\n",
    "**config.json** **pytorch_model.bin***\n",
    "\n",
    "1. **config.json** includes:\n",
    "- the **attributes** necessary to build the model architecture.   \n",
    "- some **metadata**, such as where the checkpoint originated and what ðŸ¤— Transformers version you were using when you last saved the checkpoint.       \n",
    "2. **pytorch_model.bin** \n",
    "- known as the state dictionary; \n",
    "- it contains all your **modelâ€™s weights**. \n",
    "\n",
    "**The two files go hand in hand; the configuration is necessary to know your modelâ€™s architecture, while the model weights are your modelâ€™s parameters.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Transformer model for inference\n",
    "\n",
    "After loading a modelwe can make some predictions. Transformer models can only process the  numbers that the tokenizer generates.      \n",
    "Tokenizers can take care of casting the inputs to the appropriate frameworkâ€™s tensors,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer **converts these to vocabulary indices** which are typically called **input IDs**. Each sequence is now a list of numbers! The resulting output is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sequences = [\n",
    "    [101, 7592, 999, 102],\n",
    "    [101, 4658, 1012, 102],\n",
    "    [101, 3835, 999, 102],\n",
    "]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a list of encoded sequences: a list of lists. Tensors only accept rectangular shapes (think matrices). This â€œarrayâ€ is already of rectangular shape, so converting it to a tensor is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model_inputs = torch.tensor(encoded_sequences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the tensors as inputs to the model     \n",
    "\n",
    "Making use of the tensors with the model is extremely simple â€” we just call the model with the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(model_inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers\n",
    "\n",
    "\n",
    "- Tokenizers are essential for NLP. \n",
    "- They serve one purpose: **converting text to numerical data** for models. \n",
    "- They enable processing by translating text into numbers. \n",
    "- The goal is to find the most meaningful representation â€” that is, the one that makes the most sense to the model â€” and, if possible, the smallest representation.\n",
    "\n",
    "### Some examples of tokenization algorithms:\n",
    "\n",
    "### Word-based     \n",
    "\n",
    "- Itâ€™s generally very **easy to set up** and use with only a **few rules**, \n",
    "- often yields decent results.     \n",
    "- the goal is to **split the raw text into words** and find a **numerical representation** for each of them:\n",
    "\n",
    "e.g: Let's do this!    \n",
    "    Split on spaces: Let's + do + this!\n",
    "    Split on punctuation: Let + 's + do + this + !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jim', 'Henson', 'was', 'a', 'puppeteer']\n"
     ]
    }
   ],
   "source": [
    "# use whitespace to tokenize the text into words#\n",
    "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Word tokenizers can have variations that include **extra rules for punctuation**.        \n",
    "- A **vocabulary** is defined by the **total number of independent tokens in a corpus**.          \n",
    "- **Each word** in the tokenizer **is assigned a unique ID** within the vocabulary.       \n",
    "- Completely covering a language with a word-based tokenizer **requires a large number of tokens**.      \n",
    "- **Words with similar meanings or variations** (e.g., \"dog\" and \"dogs\") may be **initially treated as unrelated** by the model.\n",
    "- A **custom token**, often represented as **\"[UNK]\"** or **\"\"**, is used to represent **words not in the vocabulary**.\n",
    "- An excessive number of unknown tokens indicates a loss of information, so the goal is to minimize the occurrence of such tokens by carefully crafting the vocabulary.\n",
    "- One way to reduce the amount of unknown tokens is to go one level deeper, using a character-based tokenizer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character-based\n",
    "\n",
    "Character-based tokenizers **split the text into characters**, rather than words. This has two primary benefits:\n",
    "\n",
    "- The vocabulary is much smaller.\n",
    "- There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters.\n",
    "\n",
    "\n",
    "- Character-based tokenization may be less meaningful compared to word-based tokenization, as individual characters carry less information.            \n",
    "- The significance of character-based tokenization varies by language, with Chinese characters holding more information than Latin characters.        \n",
    "- Character-based tokenization leads to a larger number of tokens compared to word-based tokenization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword tokenization\n",
    "\n",
    "Subword tokenization combines the advantages of both word-based and character-based approaches.\n",
    "\n",
    "Summary:\n",
    "\n",
    "- Subword tokenization algorithms follow the principle of:\n",
    "    - **frequently used words** should **not be split** into smaller subwords,       \n",
    "    - but **rare words** **should be decomposed** into meaningful subwords.                  \n",
    "    e.g: \"annoyingly\" is a rare word, can be split into subwords like \"annoying\" and \"ly\" to increase their frequency as standalone subwords while retaining the overall meaning.           \n",
    "- Subword tokenization aims to strike a **balance between preserving word meaning and efficiently representing the vocabulary**.           \n",
    "\n",
    "- An example: \"Let's do tokenization!\"      \n",
    "tokenized:  [\"Let's\", \"do\", \"token\", \"ization\", \"!\"]\n",
    "\n",
    "- Subwords obtained through tokenization carry significant **semantic meaning**.        \n",
    "- For example, \"tokenization\" can be split into \"token\" and \"ization,\" where both subwords have semantic significance.         \n",
    "- By using subwords, long words can be represented efficiently with fewer tokens.\n",
    "- Subword tokenization enables **good coverage with small vocabularies** and **minimizes the occurrence of unknown tokens.**\n",
    "- This approach is especially useful in agglutinative languages such as Turkish, where you can form (almost) arbitrarily long complex words by stringing together subwords."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more techniques out there. To name a few:        \n",
    "\n",
    "- Byte-level BPE, as used in GPT-2\n",
    "- WordPiece, as used in BERT\n",
    "- SentencePiece or Unigram, as used in several multilingual models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and saving\n",
    "\n",
    "-  itâ€™s based on the same two methods as saving and loading models: **from_pretrained()** and **save_pretrained()**.     \n",
    "- These methods will load or save: \n",
    "    - the **algorithm** used by the tokenizer (a bit like the architecture of the model),\n",
    "    - as well as **its vocabulary** (a bit like the weights of the model)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To **load the BERT tokenizer** that is **trained with the same checkpoint as BERT**, you can use the BertTokenizer class. The process is similar to loading the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Using a Transformer network is simple\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving a tokenizer** is identical to saving a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"directory_on_my_computer\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want tor know how the input_ids (in the tokenizer output) are generated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "\n",
    "- Encoding is translating **text into numerical** representations.    \n",
    "- Encoding consists of two steps: **tokenization** and **conversion to input IDs**.        \n",
    "- Tokenization **splits the text into tokens** (words, parts of words, punctuation symbols, etc.).       \n",
    "-  There are multiple rules that can govern that process, which is why we need to **instantiate the tokenizer using the name of the model,** to make sure we use the same rules that were used when the model was pretrained.            \n",
    "- Conversion of tokens to numbers involves mapping them to the **tokenizer's vocabulary**.        \n",
    "- The **tokenizer's vocabulary is downloaded during instantiation** using the `from_pretrained()` method.      \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better understanding of the two steps, weâ€™ll explore them separately. Note that we will use some methods that perform parts of the tokenization pipeline separately to show you the intermediate results of those steps, but in practice, you should call the tokenizer directly on your inputs.\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "The tokenization process is done by the tokenize() method of the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tokenizer is a **subword tokenizer**: it splits the words until it obtains tokens that can be represented by its vocabulary. Thatâ€™s the case here with transformer, which is split into two tokens: transform and ##er."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From tokens to input IDs\n",
    "\n",
    "The conversion to input IDs is handled by the convert_tokens_to_ids() tokenizer method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding\n",
    "\n",
    "Decoding is going the other way around: **from vocabulary indices**, we want to **get a string**. This can be done with the decode() method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a transformer network is simple\n"
     ]
    }
   ],
   "source": [
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decoded_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `decode` method not only converts indices back to tokens but also groups together tokens that were part of the same words, resulting in a readable sentence.\n",
    "- This behavior is particularly useful when working with models that generate new text, such as language generation or sequence-to-sequence tasks like translation or summarization.\n",
    "- The `decode` method helps in producing coherent and meaningful output by properly combining the tokens into words and sentences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling multiple sequesnces\n",
    "\n",
    "Some questions we want to answer:      \n",
    "\n",
    "1. How do we handle multiple sequences?\n",
    "2. How do we handle multiple sequences of different lengths?\n",
    "3. Are vocabulary indices the only inputs that allow a model to work well?\n",
    "4. Is there such a thing as too long a sequence?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Expect a Batch of Inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding the inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention masks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longer sequences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_HF-NLP-Notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Mar  8 2023, 04:44:36) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e271ed26a3bb9b9590c7e20b926699e380aa6724b9334935ca4b4823a968782"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
